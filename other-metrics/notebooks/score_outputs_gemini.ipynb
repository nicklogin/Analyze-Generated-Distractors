{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e6ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, warnings\n",
    "\n",
    "from ast import literal_eval\n",
    "from typing import Literal, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google import genai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data_input/data_dict.json\", 'r', encoding=\"utf8\") as inp:\n",
    "    data_dict = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c88d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_DATASET = pd.read_excel(\"../data_input/EgeEvalDataset.xlsx\")\n",
    "ORIG_DATASET[\"distractors\"] = ORIG_DATASET[\"distractors\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc603899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distractors_to_labels(distractors: list[list[str]]) -> list[Literal[0, 1]]:\n",
    "    labels = []\n",
    "\n",
    "    for distractor_set, (idx, row) in zip(distractors, ORIG_DATASET.iterrows()):\n",
    "        if row[\"question\"] == \"Какое высказывание СООТВЕТСТВУЕТ тексту?\":\n",
    "            for d in distractor_set:\n",
    "                labels.append(0)\n",
    "        elif row[\"question\"] == \"Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?\":\n",
    "            for d in distractor_set:\n",
    "                labels.append(1)\n",
    "        else:\n",
    "            raise ValueError(row[\"question\"])\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9a1056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='да'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(\"да(?![а-я])\", \"да.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055725cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"да(?![а-я])\", \"данное\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b3ada1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='да'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(\"да(?![а-я])\", \"да\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360add38",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"ENTER-YOUR-API-KEY\"\n",
    "client = genai.Client(\n",
    "    api_key=API_KEY, http_options={\"base_url\": \"ENTER-YOUR-PROVIDER-URL\"}\n",
    ")\n",
    "\n",
    "def attest_distractor_gemini(distractor: str, reading_text: str) -> Union[Literal[0, 1], None]:\n",
    "    output = None\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro-preview-05-06\",\n",
    "        contents=f\"Прочитай текст и ответь на вопрос односложно (Да или Нет). ТЕКСТ: {reading_text} ВОПРОС: ВЕРНО ЛИ СЛЕДУЮЩЕЕ УТВЕРЖДЕНИЕ? {distractor}\"\n",
    "    )\n",
    "    result = response.text\n",
    "    if re.match(\"да(?![а-я])\", result.lower().strip()):\n",
    "        output = 1\n",
    "    elif re.match(\"нет(?![а-я])\", result.lower().strip()):\n",
    "        output = 0\n",
    "    else:\n",
    "        warnings.warn(f'Got \"{result.lower().strip()}\". Setting result to None')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d34b9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60684c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "838941aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1361/1597 [3:46:55<38:42,  9.84s/it]  C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13592\\1632684005.py:18: UserWarning: Got \"чтобы ответить \"да\" или \"нет\", мне нужно само утверждение, которое я должен оценить как верное или неверное на основе текста.\n",
      "\n",
      "пожалуйста, предоставьте утверждение.\n",
      "\n",
      "если предположить, что утверждение было бы, например: \"иудин искренне сочувствует бедным и готов им помочь\", то ответ был бы **нет**. его слова расходятся с делами (он торгуется и дает меньше денег нуждающимся), а слезы названы \"крокодиловыми\".\n",
      "\n",
      "если утверждение было бы: \"иудин — лицемерный человек\", то ответ был бы **да**.\n",
      "\n",
      "без самого утверждения я не могу дать однозначный ответ.\". Setting result to None\n",
      "  warnings.warn(f'Got \"{result.lower().strip()}\". Setting result to None')\n",
      "100%|██████████| 1597/1597 [4:22:39<00:00,  9.49s/it]  "
     ]
    }
   ],
   "source": [
    "gemini_results = []\n",
    "progr = tqdm(total=sum([len(item) for k, val in data_dict.items() for item in val]))\n",
    "\n",
    "for key, val in data_dict.items():\n",
    "    val_labels = distractors_to_labels(val)\n",
    "    i = 0\n",
    "    for item, (_, orig_item) in zip(val, ORIG_DATASET.iterrows()):\n",
    "        for distractor in item:\n",
    "            gemini_results.append({\n",
    "                \"source\": key,\n",
    "                \"reading_text\": orig_item[\"reading_text\"],\n",
    "                \"question\": orig_item[\"question\"],\n",
    "                \"distractor\": distractor,\n",
    "                \"gemini_guess\": int(val_labels[i] == attest_distractor_gemini(distractor, orig_item[\"reading_text\"]))\n",
    "            })\n",
    "            i += 1\n",
    "            progr.update()\n",
    "\n",
    "gemini_results = pd.DataFrame(gemini_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e64f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_results.to_excel(\"../data_output_table/gemini_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b39e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "BartDG              0.500000\n",
       "BartDG_ANPM         0.443114\n",
       "BartDG_PM           0.461078\n",
       "ChatGPT4o           0.532934\n",
       "Deepseek            0.460606\n",
       "MuSeRC_GPT3         0.383648\n",
       "MuSeRC_T5           0.394958\n",
       "RuRace_GPT3         0.391026\n",
       "RuRace_T5           0.448485\n",
       "true_distractors    0.957831\n",
       "Name: gemini_guess, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_results.groupby(\"source\")[\"gemini_guess\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb96b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source            question                                   \n",
       "BartDG            Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.254717\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.933333\n",
       "BartDG_ANPM       Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.188679\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.885246\n",
       "BartDG_PM         Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.186916\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.950000\n",
       "ChatGPT4o         Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.299065\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.950000\n",
       "Deepseek          Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.171429\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.966667\n",
       "MuSeRC_GPT3       Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.147059\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.807018\n",
       "MuSeRC_T5         Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.213333\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.704545\n",
       "RuRace_GPT3       Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.100000\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.910714\n",
       "RuRace_T5         Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.228571\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.833333\n",
       "true_distractors  Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    0.962264\n",
       "                  Какое высказывание СООТВЕТСТВУЕТ тексту?       0.950000\n",
       "Name: gemini_guess, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_results.groupby([\"source\", \"question\"])[\"gemini_guess\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6ef8f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question\n",
       "Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?    35\n",
       "Какое высказывание СООТВЕТСТВУЕТ тексту?       20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORIG_DATASET[\"question\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env_042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
